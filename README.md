<div>
  
  <!--Header-->
  ![header](https://capsule-render.vercel.app/api?type=waving&color=gradient&height=300&section=header&text=Good%20to%20see%20you%20%F0%9F%A4%97)
  
</div>

<div>
  <!--Body-->
  
  ## üëÄ About Me
  #### :raising_hand: I am Taewook Wi from South Korea.<br/>
  #### :mortar_board: master of Smart factory convergence, Sungkyunkwan University(SKKU)
  <br/>
  <br/>
</div>

<div>
üéì Courses Taken

| Course Title                                     | Description                                                                    |
| ------------------------------------------------ | ------------------------------------------------------------------------------ |
| Smart Factory Application Programming            | Developed automation software using Python and IoT for smart manufacturing.    |
| Smart Factory Convergence Technology Seminar     | Explored interdisciplinary trends and technologies in smart factories.         |
| Applications of Deep Learning Technology         | Applied CNNs and RNNs to solve real-world classification and prediction tasks. |
| Advanced Data Analytics                          | Analyzed large-scale industrial data using statistical and ML techniques.      |
| Smart Factory Capstone Design                    | Led a hands-on project integrating smart factory components and AI models.     |
| Advanced Topics in Smart Factory Convergence     | Studied advanced integration methods of CPS, AI, and IoT in manufacturing.     |
| Introduction to Computer Vision *(Prerequisite)* | Learned foundational techniques in image processing and object detection.      |
| Thesis Writing and Research Ethics               | Trained in academic writing, citation, and ethical research conduct.           |
| Smart Factory Capstone Design II                 | Continued applied project focusing on LLM-based multimodal segmentation.       |
| Mathmatics for Machinelearning                   | Covered linear algebra, calculus, and probability for ML applications.         |
| Introduction to Data Structures *(Prerequisite)* | Explored core data structures such as arrays, lists, trees, and graphs.        |

</div>

<div>
 üìÑ Research Seminar

|Human Pose Estimation with Two-Stream Residual Steps Network|
Published in: IEEE ¬∑ Oct 2022
Date: 22 Aug 2023

Background variables like scene and camera angle in images/videos hinder accurate human pose estimation. This paper proposes a novel architecture combining RGB and MSR image features, improving HPE performance over conventional methods.

|Automatic Defect Detection in Web Offset Printing via Machine Vision|
Published in: MDPI ¬∑ Sep 2019
Date: 30 Oct 2023

Traditional printing defect detection heavily relies on human inspection, leading to inconsistency. This study introduces an image projection-based algorithm that accelerates defect localization by converting 2D searches into 1D feature matching.

|DeepLumina: Color Texture Classification using Deep Features and Luminance|
Published in: MDPI ¬∑ Apr 2022
Date: 1 Dec 2023

Human visual perception is sensitive to luminance. By incorporating YIQ luminance into deep backbone networks, this framework achieves superior texture color classification performance, integrating feature extraction and SVM classification.

|One-Shot Recognition of Steel Surface Defects|
Published in: Elsevier ¬∑ May 2020
Date: 23 Feb 2024

One-shot learning is an effective approach when annotation is costly. This research shows that a Siamese network can recognize manufacturing defects on steel surfaces with high precision, using minimal training data.

|Skin Lesion Segmentation via Attention DeepLabv3+|
Published in: Springer ¬∑ Jan 2021
Date: 12 Jul 2024

Segmenting skin lesions of various shapes and sizes is complex. This paper enhances DeepLabv3+ by introducing a multi-level attention mechanism to capture multi-scale features and improve segmentation accuracy.

|FS-MedSAM2: Few-Shot Medical Segmentation without Fine-Tuning|
Published in: arXiv ¬∑ Sep 2024
Date: 4 Oct 2024

FS-MedSAM2 utilizes the pre-trained SAM2 model for few-shot medical image segmentation without additional fine-tuning, leveraging its memory attention and prompt-based capabilities for accurate mask generation.

|LISA: Reasoning Segmentation via Large Language Model|
Published in: arXiv ¬∑ Sep 2024
Date: 28 Oct 2024

This paper introduces Reasoning Segmentation, extending referring segmentation by enabling models to comprehend implicit prompts. LISA combines LLaVA and a visual backbone to interpret abstract textual cues and generate precise segmentation masks.

|Improved Baselines with Visual Instruction Tuning|
published in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Jun 2024
Date: 09 Jan 2025

Paper Presents the first controlled study on LMM design using LLaVA. With minimal modifications and only 1.2M public samples, the model achieves SOTA on 11 benchmarks and trains in 1 day on 8 A100s.

|Eyes wide shut exploring the visual shortcomings of multimodal llms|
published in:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Sep 2024
Date: 30 Jan 2025

Reveals limitations in CLIP-based visual understanding within MLLMs. Proposes the MMVP benchmark using ‚ÄúCLIP-blind pairs‚Äù and introduces a Mixture of Features approach to enhance visual grounding by integrating self-supervised vision features.

|PSALM: Pixelwise SegmentAtion with Large Multi-modal Model|
published in:In European Conference on Computer Vision . Oct 2024
Date: 26 Mar 2025

PSALM Extends LMMs to segmentation by adding a mask decoder and task-aware input schema. Supports joint training across tasks and achieves SOTA on multiple benchmarks, while enabling zero-shot segmentation on unseen tasks like open-vocabulary and video segmentation.

|GLaMM: Pixel Grounding Large Multimodal Model|
published in:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern . 2024
Date: 03 Apr 2025

Proposes GLaMM, the first LMM to generate natural language responses with aligned segmentation masks. Accepts both text and visual prompts, enabling fine-grained multimodal interaction. Introduces the GranD datasetand defines a new task: Grounded Conversation Generation.
</div>

